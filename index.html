<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning ">
  <meta name="keywords" content="OCRBench v2">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OCRBench v2</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/web.png">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
    </div>
   
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">OCRBench v2</h1>
          <h2 class="subtitle is-3 publication-subtitle">
           OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning 
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
             Ling Fu</a><sup>1</sup>,</span>
            <span class="author-block">
             Biao Yang</a><sup>1</sup>,</span>
            <span class="author-block">
             Zhebin Kuang</a><sup>1</sup>,</span>
            <span class="author-block">
             Jiajun Song</a><sup>1</sup>,</span>
            <span class="author-block">
             Yuzhe Li</a><sup>1</sup>,</span>
            <span class="author-block">
             Linghao Zhu</a><sup>1</sup>,</span>
            <span class="author-block">
             Qidi Luo</a><sup>1</sup>,</span>
            <span class="author-block">
             Xinyu Wang</a><sup>2</sup>,</span>
            <span class="author-block">
             Hao Lu</a><sup>1</sup>,</span>
            <span class="author-block">
             Mingxin Huang</a><sup>3</sup>,</span>
            <span class="author-block">
             Zhang Li</a><sup>1</sup>,</span>
            <span class="author-block">
             Guozhi Tang</a><sup>4</sup>,</span>
            <span class="author-block">
             Bin Shan</a><sup>4</sup>,</span>
            <span class="author-block">
             Chunhui Lin</a><sup>4</sup>,</span>
            <span class="author-block">
             Qi Liu</a><sup>4</sup>,</span>
            <span class="author-block">
             Binghong Wu</a><sup>4</sup>,</span>
            <span class="author-block">
             Hao Feng</a><sup>4</sup>,</span>
            <span class="author-block">
             Hao Liu</a><sup>4</sup>,</span>
            <span class="author-block">
             Can Huang</a><sup>4</sup>,</span>
            <span class="author-block">
             Jingqun Tang</a><sup>4</sup>,</span>
            <span class="author-block">
             Wei Chen</a><sup>1</sup>,</span>
            <span class="author-block">
             Lianwen Jin</a><sup>3</sup>,</span>
            <span class="author-block">
             Yuliang Liu</a><sup>1</sup>,</span>
            <span class="author-block">
             Xiang Bai</a><sup>1</sup></span>
          </div>
          <br>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Huazhong University of Science and Technology,</span>
            <span class="author-block"><sup>2</sup>University of Adelaide,</span>
            <span class="author-block"><sup>3</sup>South China University of Technology,</span>
            <span class="author-block"><sup>4</sup>ByteDance</span>
          </div>
           <br>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2501.00321"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Yuliang-Liu/MultimodalOCR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1Hk1TMu--7nr5vJ7iaNwMQZ_Iw9W_KI3C/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
               </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/ling99/OCRBench-v2-leaderboard" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon has-text-white">
                    <i class="fa-solid fa-trophy" aria-hidden="true"></i>
                      <!-- <p style="font-size:18px">üèÜ</p> -->
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
        <div class="content has-text-centered">
          <img src="./static/images/overview.jpg" alt="overview examples" />
          <p>Scoring the Optical Character Recognition (OCR) capabilities of Large Multimodal Models (LMMs) has witnessed growing interest. Existing benchmarks have highlighted the impressive performance of LMMs in text recognition; however, their abilities in certain challenging tasks, such as text localization, handwritten content extraction, and logical reasoning, remain underexplored. To bridge this gap, we introduce OCRBench v2, a large-scale bilingual text-centric benchmark with currently the most comprehensive set of tasks (4X more tasks than the previous multi-scene benchmark OCRBench), the widest coverage of scenarios (31 diverse scenarios), and thorough evaluation metrics, with 10,000 human-verified question-answering pairs and a high proportion of difficult samples. Moreover, we construct a private test set with 1,500 manually annotated images. The consistent evaluation trends observed across both public and private test sets validate the OCRBench v2's reliability. After carefully benchmarking state-of-the-art LMMs, we find that most LMMs score below 50 (100 in total) and suffer from five-type limitations, including less frequently encountered text recognition, fine-grained perception, layout perception, complex element parsing, and logical reasoning..</p>
    </div>
  
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Leaderboard -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Leaderboard</h2>
        <div class="content has-text-justified">
          <!-- table1-->
          <p>
             Performance of LMMs on English tasks of public data
          </p>
          <div class="has-text-centered">        
            <table
              class="js-sort-table"
              id="results_with_rank"
              data-js-sort-table="true"
              width="80%"
              style="display: inline-table; font-size: 12px;"
            >
            <thead>
              <tr>
                <td class="js-sort-number" data-js-sort-colnum="0"><strong>Rank</strong></td>
                <td class="js-sort-number" data-js-sort-colnum="1"><strong>Method</strong></td>
                <td class="js-sort-number" data-js-sort-colnum="2"><strong>Recognition</strong></td>
                <td class="js-sort-number" data-js-sort-colnum="3"><strong>Referring</strong></td>
                <td class="js-sort-number" data-js-sort-colnum="4"><strong>Spotting</strong></td>
                <td class="js-sort-number" data-js-sort-colnum="5"><strong>Extraction</strong></td>
                <td class="js-sort-number" data-js-sort-colnum="6"><strong>Parsing</strong></td>
                <td class="js-sort-number" data-js-sort-colnum="7"><strong>Calculation</strong></td>
                <td class="js-sort-number" data-js-sort-colnum="8"><strong>Understanding</strong></td>
                <td class="js-sort-number" data-js-sort-colnum="9"><strong>Reasoning</strong></td>
                <td class="js-sort-number" data-js-sort-colnum="10"><strong>Average</strong></td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>1</td>
                <td>LLaVA-Next-8B</td>
                <td>41.3</td>
                <td>18.8</td>
                <td>0</td>
                <td>49.5</td>
                <td>21.2</td>
                <td>17.3</td>
                <td>55.2</td>
                <td>48.9</td>
                <td>31.5</td>
              </tr>
              <tr>
                <td>2</td>
                <td>LLaVA-OV-7B</td>
                <td>46.0</td>
                <td>20.8</td>
                <td>0.1</td>
                <td>58.3</td>
                <td>25.3</td>
                <td>23.3</td>
                <td>64.4</td>
                <td>53.0</td>
                <td>36.4</td>
              </tr>
              <tr>
                <td>3</td>
                <td>Monkey</td>
                <td>35.2</td>
                <td>0</td>
                <td>0</td>
                <td>16.6</td>
                <td>16.3</td>
                <td>14.4</td>
                <td>59.8</td>
                <td>42.3</td>
                <td>23.1</td>
              </tr>
              <tr>
                <td>4</td>
                <td>TextMonkey</td>
                <td>39.1</td>
                <td>0.7</td>
                <td>0</td>
                <td>19.0</td>
                <td>12.2</td>
                <td>19.0</td>
                <td>61.1</td>
                <td>40.2</td>
                <td>23.9</td>
              </tr>
              <tr>
                <td>5</td>
                <td>Molmo-7B</td>
                <td>52.4</td>
                <td>21.3</td>
                <td>0.1</td>
                <td>45.5</td>
                <td>7.6</td>
                <td>28.5</td>
                <td>65.3</td>
                <td>55.0</td>
                <td>34.5</td>
              </tr>
              <tr>
                <td>6</td>
                <td>Cambrian-1-8B</td>
                <td>45.3</td>
                <td>21.5</td>
                <td>0</td>
                <td>53.6</td>
                <td>19.2</td>
                <td>19.5</td>
                <td>63.5</td>
                <td>55.5</td>
                <td>34.7</td>
              </tr>
              <tr>
                <td>7</td>
                <td>Pixtral-12B</td>
                <td>48.9</td>
                <td>21.6</td>
                <td>0</td>
                <td>66.3</td>
                <td>35.5</td>
                <td>29.8</td>
                <td>66.9</td>
                <td>53.7</td>
                <td>40.3</td>
              </tr>
              <tr>
                <td>8</td>
                <td>Qwen2.5-VL-7B</td>
                <td>68.8</td>
                <td>25.7</td>
                <td>1.2</td>
                <td>80.2</td>
                <td>30.4</td>
                <td>38.2</td>
                <td>73.2</td>
                <td>56.2</td>
                <td>46.7</td>
              </tr>
              <tr>
                <td>9</td>
                <td>InternVL3-14B</td>
                <td>67.3</td>
                <td>36.9</td>
                <td>11.2</td>
                <td>89.0</td>
                <td>38.4</td>
                <td>38.4</td>
                <td>79.2</td>
                <td>60.5</td>
                <td>52.6</td>
              </tr>
              <tr>
                <td>10</td>
                <td>Deepseek-VL2-Small</td>
                <td>62.7</td>
                <td>28.0</td>
                <td>0.1</td>
                <td>77.5</td>
                <td>32.7</td>
                <td>14.3</td>
                <td>77.1</td>
                <td>53.9</td>
                <td>43.3</td>
              </tr>
              <tr>
                <td>11</td>
                <td>MiniCPM-o-2.6</td>
                <td>66.9</td>
                <td>29.5</td>
                <td>0.5</td>
                <td>70.8</td>
                <td>33.4</td>
                <td>31.9</td>
                <td>69.9</td>
                <td>57.9</td>
                <td>45.1</td>
              </tr>
              <tr>
                <td>12</td>
                <td>GLM-4V-9B</td>
                <td>61.8</td>
                <td>22.6</td>
                <td>0</td>
                <td>71.7</td>
                <td>31.6</td>
                <td>22.6</td>
                <td>72.1</td>
                <td>58.4</td>
                <td>42.6</td>
              </tr>
              <tr>
                <td>13</td>
                <td>Ovis2-8B</td>
                <td>73.2</td>
                <td>24.6</td>
                <td>0.7</td>
                <td>62.4</td>
                <td>44.8</td>
                <td>40.6</td>
                <td>72.7</td>
                <td>62.6</td>
                <td>47.7</td>
              </tr>
              <tr>
                <td>14</td>
                <td>GPT-4o</td>
                <td>61.2</td>
                <td>26.7</td>
                <td>0</td>
                <td>77.5</td>
                <td>36.3</td>
                <td>43.4</td>
                <td>71.1</td>
                <td>55.5</td>
                <td>46.5</td>
              </tr>
              <tr>
                <td>15</td>
                <td>GPT-4o-mini</td>
                <td>57.9</td>
                <td>23.3</td>
                <td>0.6</td>
                <td>70.8</td>
                <td>31.5</td>
                <td>38.8</td>
                <td>65.9</td>
                <td>55.1</td>
                <td>43.0</td>
              </tr>
              <tr>
                <td>16</td>
                <td>Gemini-Pro</td>
                <td>61.2</td>
                <td>39.5</td>
                <td>13.5</td>
                <td>79.3</td>
                <td>39.2</td>
                <td>47.7</td>
                <td>75.5</td>
                <td>59.3</td>
                <td>51.9</td>
              </tr>
              <tr>
                <td>17</td>
                <td>Claude3.5-sonnet</td>
                <td>62.2</td>
                <td>28.4</td>
                <td>1.3</td>
                <td>56.6</td>
                <td>37.8</td>
                <td>40.8</td>
                <td>73.5</td>
                <td>60.9</td>
                <td>45.2</td>
              </tr>
              <tr>
                <td>18</td>
                <td>Step-1V</td>
                <td>67.8</td>
                <td>31.3</td>
                <td>7.2</td>
                <td>73.6</td>
                <td>37.2</td>
                <td>27.8</td>
                <td>69.8</td>
                <td>58.6</td>
                <td>46.7</td>
              </tr>
            </tbody>
          </table>
        </div>
          <!-- table1-->

          
         <p>
           Performance of LMMs on Chinese tasks of public data
        </p>
        <img src="./static/images/evaluation_chinese.png" alt="Evaluation Chinese" />
         <p>
           Performance of LMMs on English tasks of private data
        </p>
        <img src="./static/images/evaluation_english_of_private_data.png" alt="Evaluation English private data" />
         <p>
           Performance of LMMs on Chinese tasks of private data
        </p>
        <img src="./static/images/evaluation_chinese_of_private_data.png" alt="Evaluation English private data" />
      </div>
    </div>
  </div>
  <!--/Leaderboard-->

  
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code></code>@misc{fu2024ocrbenchv2improvedbenchmark,
    title={OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning}, 
    author={Ling Fu and Biao Yang and Zhebin Kuang and Jiajun Song and Yuzhe Li and Linghao Zhu and Qidi Luo and Xinyu Wang and Hao Lu and Mingxin Huang and Zhang Li and Guozhi Tang and Bin Shan and Chunhui Lin and Qi Liu and Binghong Wu and Hao Feng and Hao Liu and Can Huang and Jingqun Tang and Wei Chen and Lianwen Jin and Yuliang Liu and Xiang Bai},
    year={2024},
    eprint={2501.00321},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2501.00321}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2501.00321">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Yuliang-Liu/MultimodalOCR" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a> , licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
